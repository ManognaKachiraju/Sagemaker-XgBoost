{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManognaKachiraju/Sagemaker-XgBoost/blob/main/sagemaker_implementation_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Steps To Be Followed:\n",
        "\n",
        "1) Importing necessary Libraries\n",
        "2) Creating S3 bucket\n",
        "3) Mapping train And Test Data in S3\n",
        "4) Mapping The path of the models in S3"
      ],
      "metadata": {
        "id": "K4tEGAu_SZbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) CREATING S3 BUCKETS"
      ],
      "metadata": {
        "id": "sQ6qRrJAWZTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "#boto3 can enable api to access any s3 bucket as long as its public\n",
        "from sagemaker.amazon.amazon_estimator import get_image_uri #get_image_uri to downlaod the pre-available algorithm image for our use\n",
        "from sagemaker.session import s3_input, Session #we need a session to use s3 in a sagemaker instance"
      ],
      "metadata": {
        "id": "vzcnzTnWSA-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'bankbucket' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
        "# this bucket name is just a variable not arelated to s3 bucket\n",
        "my_region = boto3.session.Session().region_name # set the region of the instance\n",
        "print(my_region)"
      ],
      "metadata": {
        "id": "UJ3KquQkSaW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#noe we use bucket name vaiable from before cell and create a bucket using code in the desired region\n",
        "s3 = boto3.resource('s3')\n",
        "try:\n",
        "    if  my_region == 'us-east-1':\n",
        "        s3.create_bucket(Bucket=bucket_name)\n",
        "    print('S3 bucket created successfully')\n",
        "except Exception as e:\n",
        "    print('S3 error: ',e)"
      ],
      "metadata": {
        "id": "Ue__Hv-NTsee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set an output path where the trained model will be saved\n",
        "prefix = 'xgboost-as-a-built-in-algo'\n",
        "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
        "# to access out bucket we need s3://bankyaswanth to access\n",
        "#here first {} is replace with bucket name and second {} with prefix name\n",
        "print(output_path)"
      ],
      "metadata": {
        "id": "dWz8yzXIVKK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) ADDING FILES IN BUCKETS"
      ],
      "metadata": {
        "id": "dV3bea_nWS3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#urllib is used to get dataset from requesting it from url and rename it to bank_clean.csv\n",
        "import urllib\n",
        "try:\n",
        "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
        "    print('Success: downloaded bank_clean.csv.')\n",
        "except Exception as e:\n",
        "    print('Data load error: ',e)\n",
        "\n",
        "try:\n",
        "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
        "    #above line we just defined the index col and created df called \"mode-data\" using the retrieved data\n",
        "    print('Success: Data loaded into dataframe.')\n",
        "except Exception as e:\n",
        "    print('Data load error: ',e)"
      ],
      "metadata": {
        "id": "pXUvFk-6WYML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Train Test split\n",
        "# instead of x_train and y_train we will split the data into just train and test where train has both x_train & y_train\n",
        "import numpy as np\n",
        "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
        "# data is split with 70% of length of data for train and 30% for testing data\n",
        "print(train_data.shape, test_data.shape)"
      ],
      "metadata": {
        "id": "SCUKDhUSXD3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Saving Train And Test Into Buckets\n",
        "## We start with Train Data\n",
        "\n",
        "#os to handle files and file paths\n",
        "import os\n",
        "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'],\n",
        "                                                axis=1)],\n",
        "                                                axis=1).to_csv('train.csv', index=False, header=False)\n",
        "# we have 2 dependent feature y_yes and y_no which are one-hot encoded so we can consider any 1 as dependent feature and drop the other\n",
        "# Above line is we editing the structure of dataframe, we add y_yes as dependent feature in front of dataframe and (index and header) = false i.e.., we drop index and header of columns\n",
        "# sagemaker dataframe default structure is y value or dependent features are in 1st col and\n",
        "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
        "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')\n",
        "# sagemaker.inputs.TrainingInput to create path in s3 to store traininf data"
      ],
      "metadata": {
        "id": "vNgGSV-tZ72B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# edit dataset structure and put Test Data Into Buckets\n",
        "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
        "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
        "s3_input_test = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')\n",
        "# even to store testing data we using sagemaker.inputs.TrainingInput which just means we create new path to store files in s3"
      ],
      "metadata": {
        "id": "7bKK2ULLaAPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Building Models using Xgboot- Inbuilt Algorithm"
      ],
      "metadata": {
        "id": "H7Y0KIPbcrta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
        "from sagemaker import image_uris\n",
        "container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name,version =\"latest\")\n",
        "# the method get_image_uri has been renamed in sagemaker>=2 See: https://sagemaker.readthedocs.io/en/stable/v2.html for details"
      ],
      "metadata": {
        "id": "UKRZUiv4cy4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# never tune hyperparams in sagemake since it's very time consuming, do them in anywhere and get the params directly into sagemaker\n",
        "# always in key value pairs this hyperparametrs is just a vriable not related to sagemaker or model\n",
        "hyperparameters = {\n",
        "        \"max_depth\":\"5\",\n",
        "        \"eta\":\"0.2\",\n",
        "        \"gamma\":\"4\",\n",
        "        \"min_child_weight\":\"6\",\n",
        "        \"subsample\":\"0.7\",\n",
        "        \"objective\":\"binary:logistic\",\n",
        "        \"num_round\":50\n",
        "        }"
      ],
      "metadata": {
        "id": "l4vliXikerrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a SageMaker estimator that calls the xgboost-container\n",
        "estimator = sagemaker.estimator.Estimator(image_uri=container,\n",
        "                                          hyperparameters=hyperparameters,\n",
        "                                          role=sagemaker.get_execution_role(), #to get iam role of the instance to access buckets\n",
        "                                          train_instance_count=1,              # train in one instance of ml.m5.2xlarge\n",
        "                                          train_instance_type='ml.m5.2xlarge',  # large to train it a bit faster\n",
        "                                          train_volume_size=5, # 5 GB\n",
        "                                          output_path=output_path, # from 4th cell\n",
        "                                          #nest 3 params are used to reduce the billing cost by 50%\n",
        "                                          train_use_spot_instances=True,\n",
        "                                          train_max_run=300,\n",
        "                                          train_max_wait=600)"
      ],
      "metadata": {
        "id": "ex4okgYafiTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator.fit({'train': s3_input_train,'validation': s3_input_test})\n",
        "# s3_input_train is training resoure path variable defined above 5th cell\n",
        "# simlilarly for s3_input_test is also a path towards s3 bucket"
      ],
      "metadata": {
        "id": "Ie7ZW6lliT6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) DEPLOY THE MODEL"
      ],
      "metadata": {
        "id": "lgAWvXX2jKiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the above will automatically save the model inside s3 bucket output folder with data and time in model name and we can retrain the model and all models are saved and sent to s3->ouputs with data and time in their name\n",
        "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n",
        "# this will deploy the model as endpoint in ml.m4.xlarge instance"
      ],
      "metadata": {
        "id": "AW-6Br9rjNq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) USING THE DEPLOYED MODEL FOR PREDICTIONS"
      ],
      "metadata": {
        "id": "JMTyDYrMkubm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.serializers import CSVSerializer # we can't directly give csv to endpoint, we need to serialize it before givng to endpoint\n",
        "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #Load the data into an array and drop dependent features and axis = 1 means we are dropping columns\n",
        "xgb_predictor.content_type = 'text/csv' # define the model input type\n",
        "xgb_predictor.serializer = CSVSerializer() # assign csvserializer as serializer for sgb_predictor\n",
        "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict  and decode (desiralize) the predictions\n",
        "predictions_array = np.fromstring(predictions[1:], sep=',') # predictions[1:] means out of all predictions we are taking the most accurate column of predictions and tuting the decoded output into an array\n",
        "print(predictions_array.shape)"
      ],
      "metadata": {
        "id": "zbstQzWNle9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_array # we are just looking over the predictions done by our model"
      ],
      "metadata": {
        "id": "n0tCAowjlkSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix code copied from aws sagemaker documentation to print out accuracy and it's confusion matrix since it's binary classification\n",
        "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
        "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
        "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
        "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
        "print(\"Observed\")\n",
        "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
        "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
      ],
      "metadata": {
        "id": "3J1aTKnulmyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) DELETING THE ENDPOINTS\n"
      ],
      "metadata": {
        "id": "FUUPBPvSlgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint) # deletes the deployed endpoint\n",
        "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name) # deletes all model, folder and data in bucket\n",
        "bucket_to_delete.objects.all().delete() # deletes all objects from buckets but not deletes bucket itself for that we have to manually delete bucket from s3"
      ],
      "metadata": {
        "id": "G21EK6hUljQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1) MANUNALLY DELETE THE BUCKET INSIDE S3\n",
        "SELECT NOTEBOOK INSTANCE -> STOP -> (AFTER IT STOPS) DELETE"
      ],
      "metadata": {
        "id": "WMgnrNwMqYr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}